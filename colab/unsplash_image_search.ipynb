{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsplash-image-search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52LIWfomlDR9"
      },
      "source": [
        "# Unsplash Joint Query Search\n",
        "\n",
        "Using this notebook you can search for images from the [Unsplash Dataset](https://unsplash.com/data) using natural language queries. The search is powered by OpenAI's [CLIP](https://github.com/openai/CLIP) neural network.\n",
        "\n",
        "This notebook uses the precomputed feature vectors for almost 2 million images from the full version of the [Unsplash Dataset](https://unsplash.com/data). If you want to compute the features yourself, see [here](https://github.com/haltakov/natural-language-image-search#on-your-machine).\n",
        "\n",
        "This project was mostly based on the [project](https://github.com/haltakov/natural-language-image-search) created by [Vladimir Haltakov](https://twitter.com/haltakov) and the full code is open-sourced on [GitHub](https://github.com/haofanwang/natural-language-joint-query-search)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-xBFnjKrmg6"
      },
      "source": [
        "!git clone https://github.com/haofanwang/natural-language-joint-query-search.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEmF8QV3roX_"
      },
      "source": [
        "!cd natural-language-joint-query-search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alp6MVBckbr-"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "In this section we will setup the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_D00etskW4a"
      },
      "source": [
        "First we need to install CLIP and then upgrade the version of torch to 1.7.1 with CUDA support (by default CLIP installs torch 1.7.1 without CUDA). Google Colab currently has torch 1.7.0 which doesn't work well with CLIP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Dj-ahDqNirPj",
        "outputId": "c8febc92-328a-4f89-c044-ba6e5e9bf2f9"
      },
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_juy7gsl\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-_juy7gsl\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from clip==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from clip==1.0) (4.41.1)\n",
            "Collecting torch~=1.7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/90/4f/acf48b3a18a8f9223c6616647f0a011a5713a985336088d7c76f3a211374/torch-1.7.1-cp36-cp36m-manylinux1_x86_64.whl (776.8MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8MB 24kB/s \n",
            "\u001b[?25hCollecting torchvision~=0.8.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/f1/d1d9b2be9f50e840accfa180ec2fb759dd2504f2b3a12a232398d5fa00ae/torchvision-0.8.2-cp36-cp36m-manylinux1_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 252kB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch~=1.7.1->clip==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch~=1.7.1->clip==1.0) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch~=1.7.1->clip==1.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision~=0.8.2->clip==1.0) (7.0.0)\n",
            "Building wheels for collected packages: clip, ftfy\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-cp36-none-any.whl size=1368563 sha256=ceda09b6709173fa540f9240e685d3f50667295685c30119743ad65bb2676d0a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-06z4kuit/wheels/79/51/d7/69f91d37121befe21d9c52332e04f592e17d1cabc7319b3e09\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp36-none-any.whl size=46451 sha256=1bc0b472127f4be66b95e619cb61f6a6bb10d803625fd98e00f14a93dad642cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built clip ftfy\n",
            "Installing collected packages: ftfy, torch, torchvision, clip\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed clip-1.0 ftfy-5.9 torch-1.7.1 torchvision-0.8.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 244kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.1\n",
            "    Uninstalling torch-1.7.1:\n",
            "      Successfully uninstalled torch-1.7.1\n",
            "  Found existing installation: torchvision 0.8.2\n",
            "    Uninstalling torchvision-0.8.2:\n",
            "      Successfully uninstalled torchvision-0.8.2\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PNNnGmzjXho"
      },
      "source": [
        "## Download the Precomputed Data\n",
        "\n",
        "In this section the precomputed feature vectors for all photos are downloaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY4-kZjTjcwH"
      },
      "source": [
        "In order to compare the photos from the Unsplash dataset to a text query, we need to compute the feature vector of each photo using CLIP. \n",
        "\n",
        "We need to download two files:\n",
        "* `photo_ids.csv` - a list of the photo IDs for all images in the dataset. The photo ID can be used to get the actual photo from Unsplash.\n",
        "* `features.npy` - a matrix containing the precomputed 512 element feature vector for each photo in the dataset.\n",
        "\n",
        "The files are available on [Google Drive](https://drive.google.com/drive/folders/1WQmedVCDIQKA2R33dkS1f980YsJXRZ-q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKDvZUY8jUbT",
        "outputId": "e2a771aa-cd96-496c-968e-aba104e7d05c"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create a folder for the precomputed features\n",
        "!mkdir unsplash-dataset\n",
        "\n",
        "# Download the photo IDs and the feature vectors\n",
        "!gdown --id 1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj -O unsplash-dataset/photo_ids.csv\n",
        "!gdown --id 1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F -O unsplash-dataset/features.npy\n",
        "\n",
        "# Download from alternative source, if the download doesn't work for some reason (for example download quota limit exceeded)\n",
        "if not Path('unsplash-dataset/photo_ids.csv').exists():\n",
        "  !wget https://transfer.army/api/download/TuWWFTe2spg/EDm6KBjc -O unsplash-dataset/photo_ids.csv\n",
        "\n",
        "if not Path('unsplash-dataset/features.npy').exists():\n",
        "  !wget https://transfer.army/api/download/LGXAaiNnMLA/AamL9PpU -O unsplash-dataset/features.npy"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj\n",
            "To: /content/drive/My Drive/natural-language-joint-query-search/unsplash-dataset/photo_ids.csv\n",
            "23.8MB [00:00, 46.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F\n",
            "To: /content/drive/My Drive/natural-language-joint-query-search/unsplash-dataset/features.npy\n",
            "2.03GB [00:41, 49.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrs6zbPtl1Tj"
      },
      "source": [
        "## Define Functions\n",
        "\n",
        "Some important functions from CLIP for processing the data are defined here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAXxDYc2mCRO"
      },
      "source": [
        "The `encode_search_query` function takes a text description and encodes it into a feature vector using the CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81gKWHmHl6C_"
      },
      "source": [
        "def encode_search_query(search_query):\n",
        "    with torch.no_grad():\n",
        "        # Encode and normalize the search query using CLIP\n",
        "        text_encoded = model.encode_text(clip.tokenize(search_query).to(device))\n",
        "        text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Retrieve the feature vector from the GPU and convert it to a numpy array\n",
        "        return text_encoded.cpu().numpy()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbFzxh_mGYf"
      },
      "source": [
        "The `find_best_matches` function compares the text feature vector to the feature vectors of all images and finds the best matches. The function returns the IDs of the best matching photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMibX7StmGgO"
      },
      "source": [
        "def find_best_matches(text_features, photo_features, photo_ids, results_count=3):\n",
        "  # Compute the similarity between the search query and each photo using the Cosine similarity\n",
        "  similarities = (photo_features @ text_features.T).squeeze(1)\n",
        "\n",
        "  # Sort the photos by their similarity score\n",
        "  best_photo_idx = (-similarities).argsort()\n",
        "\n",
        "  # Return the photo IDs of the best matches\n",
        "  return [photo_ids[i] for i in best_photo_idx[:results_count]]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLvEF3l_jSXA"
      },
      "source": [
        "We can load the pretrained public CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3xpGYiLjMbU",
        "outputId": "a819185d-21a0-4bf1-b310-a9df81823442"
      },
      "source": [
        "import clip\n",
        "import torch\n",
        "\n",
        "# Load the open CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 354M/354M [00:02<00:00, 132MiB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f8YKIsGmbVS"
      },
      "source": [
        "We can now load the pre-extracted unsplash image features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk2Va6FNjg21",
        "outputId": "b81129d3-e2a9-4b45-c3f9-c1e76018251f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the photo IDs\n",
        "photo_ids = pd.read_csv(\"unsplash-dataset/photo_ids.csv\")\n",
        "photo_ids = list(photo_ids['photo_id'])\n",
        "\n",
        "# Load the features vectors\n",
        "photo_features = np.load(\"unsplash-dataset/features.npy\")\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Photos loaded: {len(photo_ids)}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Photos loaded: 1981161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loYXPgJ2mZXY"
      },
      "source": [
        "## Search Unsplash\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMnQGKUnmu3F"
      },
      "source": [
        "Now we are ready to search the dataset using natural language. Check out the examples below and feel free to try out your own queries.\n",
        "\n",
        "In this project, we support more types of searching than the [original project](https://github.com/haltakov/natural-language-image-search).\n",
        "\n",
        "1. Text-to-Image Search\n",
        "2. Image-to-Image Search\n",
        "3. Text+Text-to-Image Search\n",
        "4. Image+Text-to-Image Search\n",
        "\n",
        "Note: \n",
        "\n",
        "1. As the Unsplash API limit is hit from time to time, we don't display the image, but show the link to download the image.\n",
        "2. As the pretrained CLIP model is mainly trained with English texts, if you want to try with different language, please use Google translation API or NMT model to translate first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4pLCN4engLt"
      },
      "source": [
        "### Text-to-Image Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hjGDf7EnmaC"
      },
      "source": [
        "#### \"Tokyo Tower at night\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBI-6Q_dnN4e",
        "outputId": "b4b453ce-3627-415a-de0e-6ba55a42a590"
      },
      "source": [
        "search_query = \"Tokyo Tower at night.\"\n",
        "\n",
        "text_features = encode_search_query(search_query)\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/36qtPbTbMBg/download\n",
            "https://unsplash.com/photos/6OfCrWyAXoE/download\n",
            "https://unsplash.com/photos/k0wtIvRgjMo/download\n",
            "https://unsplash.com/photos/fWhdOy05uZw/download\n",
            "https://unsplash.com/photos/VkJNTXa_wew/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYqTqzhBoQyj"
      },
      "source": [
        "#### \"Two children are playing in the amusement park.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfWCEtdcoYD2"
      },
      "source": [
        "search_query = \"Two children are playing in the amusement park.\"\n",
        "\n",
        "text_features = encode_search_query(search_query)\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp_7PR1Jntxb"
      },
      "source": [
        "### Image-to-Image Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9vgRWK8skjq"
      },
      "source": [
        "source_image = \"\"\n",
        "image_feature = preprocess(Image.open(search_image)).unsqueeze(0).to(device)\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(image_feature, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeBoTIwto692"
      },
      "source": [
        "### Text+Text-to-Image Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wFCicKjo9A9"
      },
      "source": [
        "search_query = \"red flower\"\n",
        "search_query_extra = \"blue sky\"\n",
        "\n",
        "text_features = encode_search_query(search_query)\n",
        "text_features_extra = encode_search_query(search_query_extra)\n",
        "\n",
        "mixed_features = text_features + text_features_extra\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(mixed_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnP34dHInxJN"
      },
      "source": [
        "### Image+Text-to-Image Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql-7AKLNpOms"
      },
      "source": [
        "search_image = \"\"\n",
        "search_text = \"red\"\n",
        "\n",
        "image_feature = preprocess(Image.open(search_image)).unsqueeze(0).to(device)\n",
        "text_feature = encode_search_query(search_text)\n",
        "\n",
        "# image + text\n",
        "modified_feature = image_feature + text_feature\n",
        "\n",
        "best_photo_ids = find_best_matches(modified_feature, photo_features, photo_ids, 10)\n",
        "    \n",
        "for photo_id in best_photo_ids:\n",
        "      print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeH__kWGtB5H"
      },
      "source": [
        "search_image = \"\"\n",
        "search_text = \"red\"\n",
        "\n",
        "image_feature = preprocess(Image.open(search_image)).unsqueeze(0).to(device)\n",
        "text_feature = encode_search_query(search_text)\n",
        "\n",
        "# image - text\n",
        "modified_feature = image_feature - text_feature\n",
        "\n",
        "best_photo_ids = find_best_matches(modified_feature, photo_features, photo_ids, 10)\n",
        "    \n",
        "for photo_id in best_photo_ids:\n",
        "      print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}