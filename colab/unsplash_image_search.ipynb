{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unsplash-image-search.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52LIWfomlDR9"
      },
      "source": [
        "# Unsplash Joint Query Search\n",
        "\n",
        "Using this notebook you can search for images from the [Unsplash Dataset](https://unsplash.com/data) using natural language queries. The search is powered by OpenAI's [CLIP](https://github.com/openai/CLIP) neural network.\n",
        "\n",
        "This notebook uses the precomputed feature vectors for almost 2 million images from the full version of the [Unsplash Dataset](https://unsplash.com/data). If you want to compute the features yourself, see [here](https://github.com/haltakov/natural-language-image-search#on-your-machine).\n",
        "\n",
        "This project was mostly based on the [project](https://github.com/haltakov/natural-language-image-search) created by [Vladimir Haltakov](https://twitter.com/haltakov) and the full code is open-sourced on [GitHub](https://github.com/haofanwang/natural-language-joint-query-search)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-xBFnjKrmg6",
        "outputId": "8e83378b-b3f3-4c89-bff0-feb84b9381f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/haofanwang/natural-language-joint-query-search.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'natural-language-joint-query-search'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 116 (delta 37), reused 43 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (116/116), 13.12 MiB | 29.52 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEmF8QV3roX_",
        "outputId": "c74fe381-2f56-4edc-f0e9-c76c9d1c734b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cd natural-language-joint-query-search"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/natural-language-joint-query-search\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Alp6MVBckbr-"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "In this section we will setup the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_D00etskW4a"
      },
      "source": [
        "First we need to install CLIP and then upgrade the version of torch to 1.7.1 with CUDA support (by default CLIP installs torch 1.7.1 without CUDA). Google Colab currently has torch 1.7.0 which doesn't work well with CLIP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj-ahDqNirPj",
        "outputId": "a73c63e4-1109-4bba-f46d-8cdc638e8ed4"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ftfy regex tqdm"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 24kB/s \n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/torchvision/\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 114kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp36-none-any.whl size=46451 sha256=9ebbd9cc943e4a7d486233233aef6bcea6db5cb3fd6f1061bf945e202d4052f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PNNnGmzjXho"
      },
      "source": [
        "## Download the Precomputed Data\n",
        "\n",
        "In this section the precomputed feature vectors for all photos are downloaded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bY4-kZjTjcwH"
      },
      "source": [
        "In order to compare the photos from the Unsplash dataset to a text query, we need to compute the feature vector of each photo using CLIP. \n",
        "\n",
        "We need to download two files:\n",
        "* `photo_ids.csv` - a list of the photo IDs for all images in the dataset. The photo ID can be used to get the actual photo from Unsplash.\n",
        "* `features.npy` - a matrix containing the precomputed 512 element feature vector for each photo in the dataset.\n",
        "\n",
        "The files are available on [Google Drive](https://drive.google.com/drive/folders/1WQmedVCDIQKA2R33dkS1f980YsJXRZ-q?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKDvZUY8jUbT",
        "outputId": "ea925a0f-7e2e-415b-eaeb-c1e4da508c3b"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create a folder for the precomputed features\n",
        "!mkdir unsplash-dataset\n",
        "\n",
        "# Download the photo IDs and the feature vectors\n",
        "!gdown --id 1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj -O unsplash-dataset/photo_ids.csv\n",
        "!gdown --id 1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F -O unsplash-dataset/features.npy\n",
        "\n",
        "# Download from alternative source, if the download doesn't work for some reason (for example download quota limit exceeded)\n",
        "if not Path('unsplash-dataset/photo_ids.csv').exists():\n",
        "  !wget https://transfer.army/api/download/TuWWFTe2spg/EDm6KBjc -O unsplash-dataset/photo_ids.csv\n",
        "\n",
        "if not Path('unsplash-dataset/features.npy').exists():\n",
        "  !wget https://transfer.army/api/download/LGXAaiNnMLA/AamL9PpU -O unsplash-dataset/features.npy"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj\n",
            "To: /content/natural-language-joint-query-search/unsplash-dataset/photo_ids.csv\n",
            "23.8MB [00:00, 111MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F\n",
            "To: /content/natural-language-joint-query-search/unsplash-dataset/features.npy\n",
            "2.03GB [00:40, 50.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrs6zbPtl1Tj"
      },
      "source": [
        "## Define Functions\n",
        "\n",
        "Some important functions from CLIP for processing the data are defined here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAXxDYc2mCRO"
      },
      "source": [
        "The `encode_search_query` function takes a text description and encodes it into a feature vector using the CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81gKWHmHl6C_"
      },
      "source": [
        "def encode_search_query(search_query):\n",
        "    with torch.no_grad():\n",
        "        # Encode and normalize the search query using CLIP\n",
        "        text_encoded, weight = model.encode_text(clip.tokenize(search_query).to(device))\n",
        "        text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Retrieve the feature vector from the GPU and convert it to a numpy array\n",
        "        return text_encoded.cpu().numpy()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMbFzxh_mGYf"
      },
      "source": [
        "The `find_best_matches` function compares the text feature vector to the feature vectors of all images and finds the best matches. The function returns the IDs of the best matching photos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMibX7StmGgO"
      },
      "source": [
        "def find_best_matches(text_features, photo_features, photo_ids, results_count=3):\n",
        "  # Compute the similarity between the search query and each photo using the Cosine similarity\n",
        "  similarities = (photo_features @ text_features.T).squeeze(1)\n",
        "\n",
        "  # Sort the photos by their similarity score\n",
        "  best_photo_idx = (-similarities).argsort()\n",
        "\n",
        "  # Return the photo IDs of the best matches\n",
        "  return [photo_ids[i] for i in best_photo_idx[:results_count]]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLvEF3l_jSXA"
      },
      "source": [
        "We can load the pretrained public CLIP model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3xpGYiLjMbU",
        "outputId": "c1e88f17-8068-48d5-9c77-67aadc2eae03"
      },
      "source": [
        "import torch\n",
        "\n",
        "from CLIP.clip import clip\n",
        "\n",
        "# Load the open CLIP model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 354M/354M [00:02<00:00, 138MiB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f8YKIsGmbVS"
      },
      "source": [
        "We can now load the pre-extracted unsplash image features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk2Va6FNjg21",
        "outputId": "38dfef8e-8e91-4290-8860-287e7076d664"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the photo IDs\n",
        "photo_ids = pd.read_csv(\"unsplash-dataset/photo_ids.csv\")\n",
        "photo_ids = list(photo_ids['photo_id'])\n",
        "\n",
        "# Load the features vectors\n",
        "photo_features = np.load(\"unsplash-dataset/features.npy\")\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Photos loaded: {len(photo_ids)}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Photos loaded: 1981161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loYXPgJ2mZXY"
      },
      "source": [
        "## Search Unsplash\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMnQGKUnmu3F"
      },
      "source": [
        "Now we are ready to search the dataset using natural language. Check out the examples below and feel free to try out your own queries.\n",
        "\n",
        "In this project, we support more types of searching than the [original project](https://github.com/haltakov/natural-language-image-search).\n",
        "\n",
        "1. Text-to-Image Search\n",
        "2. Image-to-Image Search\n",
        "3. Text+Text-to-Image Search\n",
        "4. Image+Text-to-Image Search\n",
        "\n",
        "Note: \n",
        "\n",
        "1. As the Unsplash API limit is hit from time to time, we don't display the image, but show the link to download the image.\n",
        "2. As the pretrained CLIP model is mainly trained with English texts, if you want to try with different language, please use Google translation API or NMT model to translate first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4pLCN4engLt"
      },
      "source": [
        "### Text-to-Image Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hjGDf7EnmaC"
      },
      "source": [
        "#### \"Tokyo Tower at night\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBI-6Q_dnN4e",
        "outputId": "e237fccb-63e9-427c-9d08-31dddc92e350"
      },
      "source": [
        "search_query = \"Tokyo Tower at night.\"\n",
        "\n",
        "text_features = encode_search_query(search_query)\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/Hfjoa3qqytM/download\n",
            "https://unsplash.com/photos/9tOyu48-P7M/download\n",
            "https://unsplash.com/photos/OCgMGflYgVg/download\n",
            "https://unsplash.com/photos/msYlh78QagI/download\n",
            "https://unsplash.com/photos/UYmsWq6Cf1c/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYqTqzhBoQyj"
      },
      "source": [
        "#### \"Two children are playing in the amusement park.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfWCEtdcoYD2",
        "outputId": "9ee022cb-af53-49f5-abef-5d6a9b7e4409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "search_query = \"Two children are playing in the amusement park.\"\n",
        "\n",
        "text_features = encode_search_query(search_query)\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/VPq1DiHNShY/download\n",
            "https://unsplash.com/photos/nQlKkqq6qEw/download\n",
            "https://unsplash.com/photos/lgXRsUVWl88/download\n",
            "https://unsplash.com/photos/b10qqhvwWg4/download\n",
            "https://unsplash.com/photos/xUDUhI_qsKQ/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp_7PR1Jntxb"
      },
      "source": [
        "### Image-to-Image Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9vgRWK8skjq",
        "outputId": "431cbb7e-df3f-46d5-a3d2-b6293fb8ef89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "source_image = \"./images/borna-hrzina-8IPrifbjo-0-unsplash.jpg\"\n",
        "with torch.no_grad():\n",
        "  image_feature = model.encode_image(preprocess(Image.open(source_image)).unsqueeze(0).to(device))\n",
        "  image_feature = (image_feature / image_feature.norm(dim=-1, keepdim=True)).cpu().numpy()\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(image_feature, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/8IPrifbjo-0/download\n",
            "https://unsplash.com/photos/2Hzzw1qfVTQ/download\n",
            "https://unsplash.com/photos/q1gXY48Ej78/download\n",
            "https://unsplash.com/photos/OYaw40WnhSc/download\n",
            "https://unsplash.com/photos/DpeXitxtix8/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeBoTIwto692"
      },
      "source": [
        "### Text+Text-to-Image Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wFCicKjo9A9",
        "outputId": "f98a38fc-6aea-45ca-c32b-0162babe50d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "search_query = \"red flower\"\n",
        "search_query_extra = \"blue sky\"\n",
        "\n",
        "text_features = encode_search_query(search_query)\n",
        "text_features_extra = encode_search_query(search_query_extra)\n",
        "\n",
        "mixed_features = text_features + text_features_extra\n",
        "\n",
        "# Find the best matches\n",
        "best_photo_ids = find_best_matches(mixed_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/NewdN4HJaWM/download\n",
            "https://unsplash.com/photos/r6DXsecvS4w/download\n",
            "https://unsplash.com/photos/Ye-PdCxCmEQ/download\n",
            "https://unsplash.com/photos/AFT4cSrnVZk/download\n",
            "https://unsplash.com/photos/qKBVUBtZJCU/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnP34dHInxJN"
      },
      "source": [
        "### Image+Text-to-Image Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql-7AKLNpOms",
        "outputId": "57f49fe0-146a-4557-9bba-a82e28d48ede",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "source_image = \"./images/borna-hrzina-8IPrifbjo-0-unsplash.jpg\"\n",
        "search_text = \"cars\"\n",
        "\n",
        "with torch.no_grad():\n",
        "  image_feature = model.encode_image(preprocess(Image.open(source_image)).unsqueeze(0).to(device))\n",
        "  image_feature = (image_feature / image_feature.norm(dim=-1, keepdim=True)).cpu().numpy()\n",
        "\n",
        "text_feature = encode_search_query(search_text)\n",
        "\n",
        "# image + text\n",
        "modified_feature = image_feature + text_feature\n",
        "\n",
        "best_photo_ids = find_best_matches(modified_feature, photo_features, photo_ids, 5)\n",
        "    \n",
        "for photo_id in best_photo_ids:\n",
        "      print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/8IPrifbjo-0/download\n",
            "https://unsplash.com/photos/2Hzzw1qfVTQ/download\n",
            "https://unsplash.com/photos/6FpUtZtjFjM/download\n",
            "https://unsplash.com/photos/Qm8pvpJ-uGs/download\n",
            "https://unsplash.com/photos/c3ddbxzQtdM/download\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}