{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clip-attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB5oiuiDX-_g"
      },
      "source": [
        "# Reveal the attention of CLIP\n",
        "\n",
        "In [natural-language-joint-query-search](https://github.com/haofanwang/natural-language-joint-query-search), we support for joint query search. In this project, we slightly modify the CLIP code and visualize the attention of CLIP. We can know which keywords CLIP focuses on, so as to improve the interpretability of CLIP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC9ySFtwtyS2",
        "outputId": "fca8b5e5-bb3c-4f49-ff27-5ea0d1bca6c9"
      },
      "source": [
        "!git clone https://github.com/haofanwang/natural-language-joint-query-search.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCp3GRYDTb6l",
        "outputId": "c824623b-9494-457f-ec7c-1557e459d517"
      },
      "source": [
        "cd natural-language-joint-query-search"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/natural-language-joint-query-search\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKNB0lWqZAax"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "In this section we will setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXQkjWvdt-ob",
        "outputId": "3b51132d-22b8-4107-8389-0e47a8935863"
      },
      "source": [
        "!git clone https://github.com/shashwattrivedi/Attention_visualizer.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Attention_visualizer'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 0 (delta 0), pack-reused 44\u001b[K\n",
            "Unpacking objects: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj-ahDqNirPj",
        "outputId": "a6004407-c02a-4529-ef9c-77c4456c27ef"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 246kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsJolRIz0Dqs",
        "outputId": "d8f5df28-2fa7-4746-d824-c82336dced77"
      },
      "source": [
        "!pip install ftfy"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\r\u001b[K     |█████                           | 10kB 21.0MB/s eta 0:00:01\r\u001b[K     |██████████                      | 20kB 15.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 51kB 9.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 61kB 10.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 6.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp36-none-any.whl size=46451 sha256=38350511c6b937db13fbddf3b147aae4f50b61cb83e4b92f607b6b5581e74a8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUUp8Di-ZMQb"
      },
      "source": [
        "## Loading the Precomputed Data\n",
        "\n",
        "In this section the precomputed feature vectors for all photos are loaded. About how to download the data, please refer to [natural-language-joint-query-search](https://github.com/haofanwang/natural-language-joint-query-search) or [natural-language-image-search](https://github.com/haltakov/natural-language-image-search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk2Va6FNjg21",
        "outputId": "05346cec-e9e9-4652-fdc7-c5e9c2e471e5"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the photo IDs\n",
        "photo_ids = pd.read_csv(\"unsplash-dataset/photo_ids.csv\")\n",
        "photo_ids = list(photo_ids['photo_id'])\n",
        "\n",
        "# Load the features vectors\n",
        "photo_features = np.load(\"unsplash-dataset/features.npy\")\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Photos loaded: {len(photo_ids)}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Photos loaded: 1981161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkOZDhFZHkW"
      },
      "source": [
        "## Define Functions\n",
        "\n",
        "Some important functions from CLIP for processing the data are defined here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLYbmdwgtnn"
      },
      "source": [
        "def find_best_matches(text_features, photo_features, photo_ids, results_count=3):\n",
        "  # Compute the similarity between the search query and each photo using the Cosine similarity\n",
        "  similarities = (photo_features @ text_features.T).squeeze(1)\n",
        "\n",
        "  # Sort the photos by their similarity score\n",
        "  best_photo_idx = (-similarities).argsort()\n",
        "\n",
        "  # Return the photo IDs of the best matches\n",
        "  return [photo_ids[i] for i in best_photo_idx[:results_count]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uec9kn0zZj3E"
      },
      "source": [
        "## Define Functions\n",
        "\n",
        "Load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_S1pBGqm0o4"
      },
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from CLIP.clip import clip\n",
        "from CLIP.clip import model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoUZGxy_ZoDV"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Given a search query, we first calculate its embedding and retrive images from unplash as before. Moreover, we save the weight of the last attention layer. The visualized results show the attention of CLIP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onZ42UBMbCpV"
      },
      "source": [
        "#### \"A red flower is under the blue sky and there is a bee on the flower\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjgSJq_mVKGP",
        "outputId": "8ee1013d-10f6-4e1f-d06a-f6a94b6276bf"
      },
      "source": [
        "search_query = \"A red flower is under the blue sky and there is a bee on the flower\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Encode and normalize the search query using CLIP\n",
        "    text_token = clip.tokenize(search_query).to(device)\n",
        "    text_encoded, weight = model.encode_text(text_token)\n",
        "    text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_features = text_encoded.cpu().numpy()\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/_QMxWAa3gXM/download\n",
            "https://unsplash.com/photos/lp_TphksOrg/download\n",
            "https://unsplash.com/photos/4pYmH4o0zNo/download\n",
            "https://unsplash.com/photos/Ye-PdCxCmEQ/download\n",
            "https://unsplash.com/photos/qyN7CD8qm5M/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26
        },
        "id": "FMAo8FDAa5xB",
        "outputId": "ce254c6a-601a-433d-c9f4-e4799928cb7d"
      },
      "source": [
        "from Attention_visualizer.attention_visualizer import *\n",
        "\n",
        "sentence = search_query.split(\" \")\n",
        "attention_weights = list(weight[-1][0][1+len(sentence)].cpu().numpy())[:2+len(sentence)][1:][:-1]\n",
        "attention_weights = [float(item) for item in attention_weights]\n",
        "display_attention(sentence,attention_weights)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\t<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n",
              "\t<style>\n",
              "\thtml, body {\n",
              "\tmargin: 0;\n",
              "\tpadding: 0;\n",
              "\t}\n",
              "\n",
              "\t.tooltip {\n",
              "\tposition: relative;\n",
              "\tdisplay: inline-block;\n",
              "\tborder-bottom: 1px dotted black;\n",
              "\t}\n",
              "\t.tooltip:hover .tooltiptext {\n",
              "\tvisibility: visible;\n",
              "\t}\n",
              "\n",
              "\t.tooltip .tooltiptext {\n",
              "\tvisibility: hidden;\n",
              "\tfont-size:15px;\n",
              "\twidth: 60px;\n",
              "\tbackground-color: black;\n",
              "\tcolor: #fff;\n",
              "\ttext-align: center;\n",
              "\tborder-radius: 6px;\n",
              "\tpadding: 1px 0;\n",
              "\tposition: absolute;\n",
              "\n",
              "\tleft: 50%;\n",
              "\tmargin-left: -60px;\n",
              "\t}\n",
              "\n",
              "\t</style>\n",
              "\t<script>\n",
              "\tvar dataset =[[\"This\",0.4],[\"is\",0.3],[\"a\",0.2],[\"text\",0.4]]\n",
              "\tvar seconddataset = [\"Hello\",\"World\"]\n",
              "\n",
              "\tfunction float2int (value) {\n",
              "\treturn value | 0;\n",
              "\t}\n",
              "\n",
              "\tfunction tohex(fraction)\n",
              "\t{\n",
              "\tvar value = float2int(255 * fraction);\n",
              "\tif(value==0)\n",
              "\treturn \"00\"\n",
              "\tvar mapping = ['A','B','C','D','E','F'];\n",
              "\tvar hex=\"\";\n",
              "\n",
              "\twhile(value!==0)\n",
              "\t{\n",
              "\tvar curr= value%16;\n",
              "\tif(curr >9)\n",
              "\t  {\n",
              "\t    hex = mapping[curr-10] + hex;\n",
              "\t  }\n",
              "\telse\n",
              "\t  hex = curr + hex;\n",
              "\tvalue = float2int(value/16);\n",
              "\t}\n",
              "\n",
              "\treturn hex;\n",
              "\t}\n",
              "\n",
              "\td3.select('#text')\n",
              "\t.selectAll('#text')\n",
              "\t.data([{\"token\": \"A\", \"weight\": 0.003204345703125}, {\"token\": \"red\", \"weight\": 0.0301055908203125}, {\"token\": \"flower\", \"weight\": 0.09893798828125}, {\"token\": \"is\", \"weight\": 0.02557373046875}, {\"token\": \"under\", \"weight\": 0.00525665283203125}, {\"token\": \"the\", \"weight\": 0.014068603515625}, {\"token\": \"blue\", \"weight\": 0.059783935546875}, {\"token\": \"sky\", \"weight\": 0.1549072265625}, {\"token\": \"and\", \"weight\": 0.0220794677734375}, {\"token\": \"there\", \"weight\": 0.0172271728515625}, {\"token\": \"is\", \"weight\": 0.000797271728515625}, {\"token\": \"a\", \"weight\": 0.0009851455688476562}, {\"token\": \"bee\", \"weight\": 0.1273193359375}, {\"token\": \"on\", \"weight\": 0.00838470458984375}, {\"token\": \"the\", \"weight\": 0.0019073486328125}, {\"token\": \"flower\", \"weight\": 0.039276123046875}])\n",
              "\t.enter()\n",
              "\t.append('tspan')\n",
              "\t.style('font-family','verdana')\n",
              "\t.style('background-color', function(d,i){return '#FF' +tohex(1-d.weight) +tohex(1-d.weight) ;})\n",
              "\t.style('margin','2px')\n",
              "\t.attr(\"class\",\"tooltip\")\n",
              "\t.style('font-size',function(d){return  18 +0*d.weight + \"px\" ;})\n",
              "\t.attr(\"onmouseover\", \"handleMouseOver()\")\n",
              "\t.text(function(d){\n",
              "\treturn d.token+\" \" ;\n",
              "\t})\n",
              "\t.append('span')\n",
              "\t.attr('class',\"tooltiptext\")\n",
              "\t.text(function(d){\n",
              "\treturn Math.round(d.weight*10000)/100;\n",
              "\t});\n",
              "\t</script>\n",
              "\t<div id = 'text' style=\"margin-left:50px;\"></div>\n",
              "\t"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SvR1jbUbFc-"
      },
      "source": [
        "#### \"Two dogs playing in the snow\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBuB7SAWXTMr",
        "outputId": "026fb88a-83e8-44fb-d5b5-303e50668232"
      },
      "source": [
        "search_query = \"a woman holding an umbrella standing next to a man in a rainy day\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Encode and normalize the search query using CLIP\n",
        "    text_token = clip.tokenize(search_query).to(device)\n",
        "    text_encoded, weight = model.encode_text(text_token)\n",
        "    text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_features = text_encoded.cpu().numpy()\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/EFOYS783_D0/download\n",
            "https://unsplash.com/photos/KKDOB6YLZtM/download\n",
            "https://unsplash.com/photos/qNo7I5cbZKg/download\n",
            "https://unsplash.com/photos/cNgiyFNlZw8/download\n",
            "https://unsplash.com/photos/AVQRYiyXO7o/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26
        },
        "id": "lh9acvp7bOLD",
        "outputId": "1eec4820-7c06-438a-870d-c199f49b9be1"
      },
      "source": [
        "from Attention_visualizer.attention_visualizer import *\n",
        "\n",
        "sentence = search_query.split(\" \")\n",
        "attention_weights = list(weight[-1][0][1+len(sentence)].cpu().numpy())[:2+len(sentence)][1:][:-1]\n",
        "attention_weights = [float(item) for item in attention_weights]\n",
        "display_attention(sentence,attention_weights)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\t<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n",
              "\t<style>\n",
              "\thtml, body {\n",
              "\tmargin: 0;\n",
              "\tpadding: 0;\n",
              "\t}\n",
              "\n",
              "\t.tooltip {\n",
              "\tposition: relative;\n",
              "\tdisplay: inline-block;\n",
              "\tborder-bottom: 1px dotted black;\n",
              "\t}\n",
              "\t.tooltip:hover .tooltiptext {\n",
              "\tvisibility: visible;\n",
              "\t}\n",
              "\n",
              "\t.tooltip .tooltiptext {\n",
              "\tvisibility: hidden;\n",
              "\tfont-size:15px;\n",
              "\twidth: 60px;\n",
              "\tbackground-color: black;\n",
              "\tcolor: #fff;\n",
              "\ttext-align: center;\n",
              "\tborder-radius: 6px;\n",
              "\tpadding: 1px 0;\n",
              "\tposition: absolute;\n",
              "\n",
              "\tleft: 50%;\n",
              "\tmargin-left: -60px;\n",
              "\t}\n",
              "\n",
              "\t</style>\n",
              "\t<script>\n",
              "\tvar dataset =[[\"This\",0.4],[\"is\",0.3],[\"a\",0.2],[\"text\",0.4]]\n",
              "\tvar seconddataset = [\"Hello\",\"World\"]\n",
              "\n",
              "\tfunction float2int (value) {\n",
              "\treturn value | 0;\n",
              "\t}\n",
              "\n",
              "\tfunction tohex(fraction)\n",
              "\t{\n",
              "\tvar value = float2int(255 * fraction);\n",
              "\tif(value==0)\n",
              "\treturn \"00\"\n",
              "\tvar mapping = ['A','B','C','D','E','F'];\n",
              "\tvar hex=\"\";\n",
              "\n",
              "\twhile(value!==0)\n",
              "\t{\n",
              "\tvar curr= value%16;\n",
              "\tif(curr >9)\n",
              "\t  {\n",
              "\t    hex = mapping[curr-10] + hex;\n",
              "\t  }\n",
              "\telse\n",
              "\t  hex = curr + hex;\n",
              "\tvalue = float2int(value/16);\n",
              "\t}\n",
              "\n",
              "\treturn hex;\n",
              "\t}\n",
              "\n",
              "\td3.select('#text')\n",
              "\t.selectAll('#text')\n",
              "\t.data([{\"token\": \"a\", \"weight\": 0.002105712890625}, {\"token\": \"woman\", \"weight\": 0.00774383544921875}, {\"token\": \"holding\", \"weight\": 0.01201629638671875}, {\"token\": \"an\", \"weight\": 0.0027790069580078125}, {\"token\": \"umbrella\", \"weight\": 0.0902099609375}, {\"token\": \"standing\", \"weight\": 0.043701171875}, {\"token\": \"next\", \"weight\": 0.0160064697265625}, {\"token\": \"to\", \"weight\": 0.0219879150390625}, {\"token\": \"a\", \"weight\": 0.0034332275390625}, {\"token\": \"man\", \"weight\": 0.0139617919921875}, {\"token\": \"in\", \"weight\": 0.013519287109375}, {\"token\": \"a\", \"weight\": 0.005016326904296875}, {\"token\": \"rainy\", \"weight\": 0.04620361328125}, {\"token\": \"day\", \"weight\": 0.032440185546875}])\n",
              "\t.enter()\n",
              "\t.append('tspan')\n",
              "\t.style('font-family','verdana')\n",
              "\t.style('background-color', function(d,i){return '#FF' +tohex(1-d.weight) +tohex(1-d.weight) ;})\n",
              "\t.style('margin','2px')\n",
              "\t.attr(\"class\",\"tooltip\")\n",
              "\t.style('font-size',function(d){return  18 +0*d.weight + \"px\" ;})\n",
              "\t.attr(\"onmouseover\", \"handleMouseOver()\")\n",
              "\t.text(function(d){\n",
              "\treturn d.token+\" \" ;\n",
              "\t})\n",
              "\t.append('span')\n",
              "\t.attr('class',\"tooltiptext\")\n",
              "\t.text(function(d){\n",
              "\treturn Math.round(d.weight*10000)/100;\n",
              "\t});\n",
              "\t</script>\n",
              "\t<div id = 'text' style=\"margin-left:50px;\"></div>\n",
              "\t"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}