{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "clip-attention.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB5oiuiDX-_g"
      },
      "source": [
        "# Reveal the attention of CLIP\n",
        "\n",
        "In [natural-language-joint-query-search](https://github.com/haofanwang/natural-language-joint-query-search), we support for joint query search. In this project, we slightly modify the CLIP code and visualize the attention of CLIP. We can know which keywords CLIP focuses on, so as to improve the interpretability of CLIP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC9ySFtwtyS2",
        "outputId": "dd4c0510-817b-4f07-87c1-c50e1f56c583"
      },
      "source": [
        "!git clone https://github.com/haofanwang/natural-language-joint-query-search.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'natural-language-joint-query-search'...\n",
            "remote: Enumerating objects: 116, done.\u001b[K\n",
            "remote: Counting objects: 100% (116/116), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 116 (delta 37), reused 43 (delta 5), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (116/116), 13.12 MiB | 29.58 MiB/s, done.\n",
            "Resolving deltas: 100% (37/37), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCp3GRYDTb6l",
        "outputId": "6d29aeb0-079a-4b08-d162-4c0c2cebb8e9"
      },
      "source": [
        "cd natural-language-joint-query-search"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/natural-language-joint-query-search\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKNB0lWqZAax"
      },
      "source": [
        "## Setup Environment\n",
        "\n",
        "In this section we will setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXQkjWvdt-ob",
        "outputId": "fec01e9a-5e8c-49a7-9254-c095ab7ef944"
      },
      "source": [
        "!git clone https://github.com/shashwattrivedi/Attention_visualizer.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Attention_visualizer'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Total 44 (delta 0), reused 0 (delta 0), pack-reused 44\u001b[K\n",
            "Unpacking objects:   2% (1/44)   \rUnpacking objects:   4% (2/44)   \rUnpacking objects:   6% (3/44)   \rUnpacking objects:   9% (4/44)   \rUnpacking objects:  11% (5/44)   \rUnpacking objects:  13% (6/44)   \rUnpacking objects:  15% (7/44)   \rUnpacking objects:  18% (8/44)   \rUnpacking objects:  20% (9/44)   \rUnpacking objects:  22% (10/44)   \rUnpacking objects:  25% (11/44)   \rUnpacking objects:  27% (12/44)   \rUnpacking objects:  29% (13/44)   \rUnpacking objects:  31% (14/44)   \rUnpacking objects:  34% (15/44)   \rUnpacking objects:  36% (16/44)   \rUnpacking objects:  38% (17/44)   \rUnpacking objects:  40% (18/44)   \rUnpacking objects:  43% (19/44)   \rUnpacking objects:  45% (20/44)   \rUnpacking objects:  47% (21/44)   \rUnpacking objects:  50% (22/44)   \rUnpacking objects:  52% (23/44)   \rUnpacking objects:  54% (24/44)   \rUnpacking objects:  56% (25/44)   \rUnpacking objects:  59% (26/44)   \rUnpacking objects:  61% (27/44)   \rUnpacking objects:  63% (28/44)   \rUnpacking objects:  65% (29/44)   \rUnpacking objects:  68% (30/44)   \rUnpacking objects:  70% (31/44)   \rUnpacking objects:  72% (32/44)   \rUnpacking objects:  75% (33/44)   \rUnpacking objects:  77% (34/44)   \rUnpacking objects:  79% (35/44)   \rUnpacking objects:  81% (36/44)   \rUnpacking objects:  84% (37/44)   \rUnpacking objects:  86% (38/44)   \rUnpacking objects:  88% (39/44)   \rUnpacking objects:  90% (40/44)   \rUnpacking objects:  93% (41/44)   \rUnpacking objects:  95% (42/44)   \rUnpacking objects:  97% (43/44)   \rUnpacking objects: 100% (44/44)   \rUnpacking objects: 100% (44/44), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj-ahDqNirPj",
        "outputId": "ad9a7505-8568-477f-ed4c-a4086f4c6d2e"
      },
      "source": [
        "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install ftfy regex tqdm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.7.1+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp36-cp36m-linux_x86_64.whl (735.4MB)\n",
            "\u001b[K     |████████████████████████████████| 735.4MB 25kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
            "\u001b[?25l  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp36-cp36m-linux_x86_64.whl (12.8MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8MB 253kB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.8.2+cu101) (7.0.0)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.7.0+cu101\n",
            "    Uninstalling torch-1.7.0+cu101:\n",
            "      Successfully uninstalled torch-1.7.0+cu101\n",
            "  Found existing installation: torchvision 0.8.1+cu101\n",
            "    Uninstalling torchvision-0.8.1+cu101:\n",
            "      Successfully uninstalled torchvision-0.8.1+cu101\n",
            "Successfully installed torch-1.7.1+cu101 torchvision-0.8.2+cu101\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/06/e5c80e2e0f979628d47345efba51f7ba386fe95963b11c594209085f5a9b/ftfy-5.9.tar.gz (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.9-cp36-none-any.whl size=46451 sha256=2509dd2c203703b08dc2264b0872af6912ff2aa237aca7c0128c0dd4771d4087\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/2e/f0/b07196e8c929114998f0316894a61c752b63bfa3fdd50d2fc3\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-5.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUUp8Di-ZMQb"
      },
      "source": [
        "## Loading the Precomputed Data\n",
        "\n",
        "In this section the precomputed feature vectors for all photos are loaded. About how to download the data, please refer to [natural-language-joint-query-search](https://github.com/haofanwang/natural-language-joint-query-search) or [natural-language-image-search](https://github.com/haltakov/natural-language-image-search)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sOJECBc2lWw",
        "outputId": "6d14b909-fea3-4fa9-e128-0bbf2f572cbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Create a folder for the precomputed features\n",
        "!mkdir unsplash-dataset\n",
        "\n",
        "# Download the photo IDs and the feature vectors\n",
        "!gdown --id 1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj -O unsplash-dataset/photo_ids.csv\n",
        "!gdown --id 1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F -O unsplash-dataset/features.npy\n",
        "\n",
        "# Download from alternative source, if the download doesn't work for some reason (for example download quota limit exceeded)\n",
        "if not Path('unsplash-dataset/photo_ids.csv').exists():\n",
        "  !wget https://transfer.army/api/download/TuWWFTe2spg/EDm6KBjc -O unsplash-dataset/photo_ids.csv\n",
        "\n",
        "if not Path('unsplash-dataset/features.npy').exists():\n",
        "  !wget https://transfer.army/api/download/LGXAaiNnMLA/AamL9PpU -O unsplash-dataset/features.npy"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FdmDEzBQCf3OxqY9SbU-jLfH_yZ6UPSj\n",
            "To: /content/natural-language-joint-query-search/unsplash-dataset/photo_ids.csv\n",
            "23.8MB [00:00, 65.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L7ulhn4VeN-2aOM-fYmljza_TQok-j9F\n",
            "To: /content/natural-language-joint-query-search/unsplash-dataset/features.npy\n",
            "2.03GB [00:18, 108MB/s] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zk2Va6FNjg21",
        "outputId": "675da8d5-101d-40c0-b29f-a720931509f9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the photo IDs\n",
        "photo_ids = pd.read_csv(\"unsplash-dataset/photo_ids.csv\")\n",
        "photo_ids = list(photo_ids['photo_id'])\n",
        "\n",
        "# Load the features vectors\n",
        "photo_features = np.load(\"unsplash-dataset/features.npy\")\n",
        "\n",
        "# Print some statistics\n",
        "print(f\"Photos loaded: {len(photo_ids)}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Photos loaded: 1981161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCkOZDhFZHkW"
      },
      "source": [
        "## Define Functions\n",
        "\n",
        "Some important functions from CLIP for processing the data are defined here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSLYbmdwgtnn"
      },
      "source": [
        "def find_best_matches(text_features, photo_features, photo_ids, results_count=3):\n",
        "  # Compute the similarity between the search query and each photo using the Cosine similarity\n",
        "  similarities = (photo_features @ text_features.T).squeeze(1)\n",
        "\n",
        "  # Sort the photos by their similarity score\n",
        "  best_photo_idx = (-similarities).argsort()\n",
        "\n",
        "  # Return the photo IDs of the best matches\n",
        "  return [photo_ids[i] for i in best_photo_idx[:results_count]]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uec9kn0zZj3E"
      },
      "source": [
        "## Define Functions\n",
        "\n",
        "Load the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_S1pBGqm0o4",
        "outputId": "a8f3f7b4-160e-4e64-d0d6-4641f5caed5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "from CLIP.clip import clip\n",
        "from CLIP.clip import model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 354M/354M [00:02<00:00, 137MiB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoUZGxy_ZoDV"
      },
      "source": [
        "## Visualization\n",
        "\n",
        "Given a search query, we first calculate its embedding and retrive images from unplash as before. Moreover, we save the weight of the last attention layer. The visualized results show the attention of CLIP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onZ42UBMbCpV"
      },
      "source": [
        "#### \"A red flower is under the blue sky and there is a bee on the flower\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjgSJq_mVKGP",
        "outputId": "7ba489ae-ae20-4637-e556-2b9066f5d0a5"
      },
      "source": [
        "search_query = \"A red flower is under the blue sky and there is a bee on the flower\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Encode and normalize the search query using CLIP\n",
        "    text_token = clip.tokenize(search_query).to(device)\n",
        "    text_encoded, weight = model.encode_text(text_token)\n",
        "    text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_features = text_encoded.cpu().numpy()\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/_QMxWAa3gXM/download\n",
            "https://unsplash.com/photos/lp_TphksOrg/download\n",
            "https://unsplash.com/photos/4pYmH4o0zNo/download\n",
            "https://unsplash.com/photos/Ye-PdCxCmEQ/download\n",
            "https://unsplash.com/photos/qyN7CD8qm5M/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26
        },
        "id": "FMAo8FDAa5xB",
        "outputId": "c30f31a0-4d80-431b-82c7-f141d1209fb3"
      },
      "source": [
        "from Attention_visualizer.attention_visualizer import *\n",
        "\n",
        "sentence = search_query.split(\" \")\n",
        "attention_weights = list(weight[-1][0][1+len(sentence)].cpu().numpy())[:2+len(sentence)][1:][:-1]\n",
        "attention_weights = [float(item) for item in attention_weights]\n",
        "display_attention(sentence,attention_weights)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\t<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n",
              "\t<style>\n",
              "\thtml, body {\n",
              "\tmargin: 0;\n",
              "\tpadding: 0;\n",
              "\t}\n",
              "\n",
              "\t.tooltip {\n",
              "\tposition: relative;\n",
              "\tdisplay: inline-block;\n",
              "\tborder-bottom: 1px dotted black;\n",
              "\t}\n",
              "\t.tooltip:hover .tooltiptext {\n",
              "\tvisibility: visible;\n",
              "\t}\n",
              "\n",
              "\t.tooltip .tooltiptext {\n",
              "\tvisibility: hidden;\n",
              "\tfont-size:15px;\n",
              "\twidth: 60px;\n",
              "\tbackground-color: black;\n",
              "\tcolor: #fff;\n",
              "\ttext-align: center;\n",
              "\tborder-radius: 6px;\n",
              "\tpadding: 1px 0;\n",
              "\tposition: absolute;\n",
              "\n",
              "\tleft: 50%;\n",
              "\tmargin-left: -60px;\n",
              "\t}\n",
              "\n",
              "\t</style>\n",
              "\t<script>\n",
              "\tvar dataset =[[\"This\",0.4],[\"is\",0.3],[\"a\",0.2],[\"text\",0.4]]\n",
              "\tvar seconddataset = [\"Hello\",\"World\"]\n",
              "\n",
              "\tfunction float2int (value) {\n",
              "\treturn value | 0;\n",
              "\t}\n",
              "\n",
              "\tfunction tohex(fraction)\n",
              "\t{\n",
              "\tvar value = float2int(255 * fraction);\n",
              "\tif(value==0)\n",
              "\treturn \"00\"\n",
              "\tvar mapping = ['A','B','C','D','E','F'];\n",
              "\tvar hex=\"\";\n",
              "\n",
              "\twhile(value!==0)\n",
              "\t{\n",
              "\tvar curr= value%16;\n",
              "\tif(curr >9)\n",
              "\t  {\n",
              "\t    hex = mapping[curr-10] + hex;\n",
              "\t  }\n",
              "\telse\n",
              "\t  hex = curr + hex;\n",
              "\tvalue = float2int(value/16);\n",
              "\t}\n",
              "\n",
              "\treturn hex;\n",
              "\t}\n",
              "\n",
              "\td3.select('#text')\n",
              "\t.selectAll('#text')\n",
              "\t.data([{\"token\": \"A\", \"weight\": 0.0032100677490234375}, {\"token\": \"red\", \"weight\": 0.0301055908203125}, {\"token\": \"flower\", \"weight\": 0.0987548828125}, {\"token\": \"is\", \"weight\": 0.02557373046875}, {\"token\": \"under\", \"weight\": 0.005268096923828125}, {\"token\": \"the\", \"weight\": 0.01404571533203125}, {\"token\": \"blue\", \"weight\": 0.05975341796875}, {\"token\": \"sky\", \"weight\": 0.155029296875}, {\"token\": \"and\", \"weight\": 0.0221099853515625}, {\"token\": \"there\", \"weight\": 0.0172119140625}, {\"token\": \"is\", \"weight\": 0.0008020401000976562}, {\"token\": \"a\", \"weight\": 0.0009870529174804688}, {\"token\": \"bee\", \"weight\": 0.12744140625}, {\"token\": \"on\", \"weight\": 0.008392333984375}, {\"token\": \"the\", \"weight\": 0.0019006729125976562}, {\"token\": \"flower\", \"weight\": 0.039306640625}])\n",
              "\t.enter()\n",
              "\t.append('tspan')\n",
              "\t.style('font-family','verdana')\n",
              "\t.style('background-color', function(d,i){return '#FF' +tohex(1-d.weight) +tohex(1-d.weight) ;})\n",
              "\t.style('margin','2px')\n",
              "\t.attr(\"class\",\"tooltip\")\n",
              "\t.style('font-size',function(d){return  18 +0*d.weight + \"px\" ;})\n",
              "\t.attr(\"onmouseover\", \"handleMouseOver()\")\n",
              "\t.text(function(d){\n",
              "\treturn d.token+\" \" ;\n",
              "\t})\n",
              "\t.append('span')\n",
              "\t.attr('class',\"tooltiptext\")\n",
              "\t.text(function(d){\n",
              "\treturn Math.round(d.weight*10000)/100;\n",
              "\t});\n",
              "\t</script>\n",
              "\t<div id = 'text' style=\"margin-left:50px;\"></div>\n",
              "\t"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SvR1jbUbFc-"
      },
      "source": [
        "#### \"A woman holding an umbrella standing next to a man in a rainy day\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBuB7SAWXTMr",
        "outputId": "7033b1aa-75b9-4854-a30a-ddb5912adb41"
      },
      "source": [
        "search_query = \"A woman holding an umbrella standing next to a man in a rainy day\"\n",
        "\n",
        "with torch.no_grad():\n",
        "    # Encode and normalize the search query using CLIP\n",
        "    text_token = clip.tokenize(search_query).to(device)\n",
        "    text_encoded, weight = model.encode_text(text_token)\n",
        "    text_encoded /= text_encoded.norm(dim=-1, keepdim=True)\n",
        "\n",
        "text_features = text_encoded.cpu().numpy()\n",
        "best_photo_ids = find_best_matches(text_features, photo_features, photo_ids, 5)\n",
        "\n",
        "for photo_id in best_photo_ids:\n",
        "  print(\"https://unsplash.com/photos/{}/download\".format(photo_id))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://unsplash.com/photos/EFOYS783_D0/download\n",
            "https://unsplash.com/photos/KKDOB6YLZtM/download\n",
            "https://unsplash.com/photos/qNo7I5cbZKg/download\n",
            "https://unsplash.com/photos/cNgiyFNlZw8/download\n",
            "https://unsplash.com/photos/AVQRYiyXO7o/download\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26
        },
        "id": "lh9acvp7bOLD",
        "outputId": "dfd0b0c6-eeb5-4a37-b1f2-e3048d6f8c70"
      },
      "source": [
        "from Attention_visualizer.attention_visualizer import *\n",
        "\n",
        "sentence = search_query.split(\" \")\n",
        "attention_weights = list(weight[-1][0][1+len(sentence)].cpu().numpy())[:2+len(sentence)][1:][:-1]\n",
        "attention_weights = [float(item) for item in attention_weights]\n",
        "display_attention(sentence,attention_weights)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "\t<script src=\"https://d3js.org/d3.v4.min.js\"></script>\n",
              "\t<style>\n",
              "\thtml, body {\n",
              "\tmargin: 0;\n",
              "\tpadding: 0;\n",
              "\t}\n",
              "\n",
              "\t.tooltip {\n",
              "\tposition: relative;\n",
              "\tdisplay: inline-block;\n",
              "\tborder-bottom: 1px dotted black;\n",
              "\t}\n",
              "\t.tooltip:hover .tooltiptext {\n",
              "\tvisibility: visible;\n",
              "\t}\n",
              "\n",
              "\t.tooltip .tooltiptext {\n",
              "\tvisibility: hidden;\n",
              "\tfont-size:15px;\n",
              "\twidth: 60px;\n",
              "\tbackground-color: black;\n",
              "\tcolor: #fff;\n",
              "\ttext-align: center;\n",
              "\tborder-radius: 6px;\n",
              "\tpadding: 1px 0;\n",
              "\tposition: absolute;\n",
              "\n",
              "\tleft: 50%;\n",
              "\tmargin-left: -60px;\n",
              "\t}\n",
              "\n",
              "\t</style>\n",
              "\t<script>\n",
              "\tvar dataset =[[\"This\",0.4],[\"is\",0.3],[\"a\",0.2],[\"text\",0.4]]\n",
              "\tvar seconddataset = [\"Hello\",\"World\"]\n",
              "\n",
              "\tfunction float2int (value) {\n",
              "\treturn value | 0;\n",
              "\t}\n",
              "\n",
              "\tfunction tohex(fraction)\n",
              "\t{\n",
              "\tvar value = float2int(255 * fraction);\n",
              "\tif(value==0)\n",
              "\treturn \"00\"\n",
              "\tvar mapping = ['A','B','C','D','E','F'];\n",
              "\tvar hex=\"\";\n",
              "\n",
              "\twhile(value!==0)\n",
              "\t{\n",
              "\tvar curr= value%16;\n",
              "\tif(curr >9)\n",
              "\t  {\n",
              "\t    hex = mapping[curr-10] + hex;\n",
              "\t  }\n",
              "\telse\n",
              "\t  hex = curr + hex;\n",
              "\tvalue = float2int(value/16);\n",
              "\t}\n",
              "\n",
              "\treturn hex;\n",
              "\t}\n",
              "\n",
              "\td3.select('#text')\n",
              "\t.selectAll('#text')\n",
              "\t.data([{\"token\": \"A\", \"weight\": 0.0021076202392578125}, {\"token\": \"woman\", \"weight\": 0.0077362060546875}, {\"token\": \"holding\", \"weight\": 0.01203155517578125}, {\"token\": \"an\", \"weight\": 0.002777099609375}, {\"token\": \"umbrella\", \"weight\": 0.09014892578125}, {\"token\": \"standing\", \"weight\": 0.043701171875}, {\"token\": \"next\", \"weight\": 0.0159912109375}, {\"token\": \"to\", \"weight\": 0.0220489501953125}, {\"token\": \"a\", \"weight\": 0.0034427642822265625}, {\"token\": \"man\", \"weight\": 0.0139617919921875}, {\"token\": \"in\", \"weight\": 0.0135345458984375}, {\"token\": \"a\", \"weight\": 0.0050201416015625}, {\"token\": \"rainy\", \"weight\": 0.046234130859375}, {\"token\": \"day\", \"weight\": 0.032379150390625}])\n",
              "\t.enter()\n",
              "\t.append('tspan')\n",
              "\t.style('font-family','verdana')\n",
              "\t.style('background-color', function(d,i){return '#FF' +tohex(1-d.weight) +tohex(1-d.weight) ;})\n",
              "\t.style('margin','2px')\n",
              "\t.attr(\"class\",\"tooltip\")\n",
              "\t.style('font-size',function(d){return  18 +0*d.weight + \"px\" ;})\n",
              "\t.attr(\"onmouseover\", \"handleMouseOver()\")\n",
              "\t.text(function(d){\n",
              "\treturn d.token+\" \" ;\n",
              "\t})\n",
              "\t.append('span')\n",
              "\t.attr('class',\"tooltiptext\")\n",
              "\t.text(function(d){\n",
              "\treturn Math.round(d.weight*10000)/100;\n",
              "\t});\n",
              "\t</script>\n",
              "\t<div id = 'text' style=\"margin-left:50px;\"></div>\n",
              "\t"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}